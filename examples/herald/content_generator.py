#!/usr/bin/env python3
"""
HERALD Content Generator
Transforms research findings into platform-optimized content

Supports two modes:
1. Rotation Mode: Pre-written STEWARD Protocol messages (fallback)
2. Research Mode: LLM-powered analysis of trending topics + intelligent hot takes

Uses OpenRouter for cost-effective LLM access (respects Cognitive Policy budget)
"""

import os
from datetime import datetime
from typing import Dict, Optional


class ContentGenerator:
    """
    Transform research findings or rotation topics into social media content.

    Modes:
    - rotation: Use pre-written topic rotation (fast, no API cost)
    - research: Use LLM to generate hot take on research finding (intelligent)
    - hybrid: Try research first, fallback to rotation if API unavailable
    """

    # Pre-written rotation content (fallback)
    ROTATION_TOPICS = [
        {
            "title": "Why AI Agents Need Cryptographic Identity",
            "twitter": "Did you know? Most AI agents have NO verifiable identity. Enter STEWARD Protocol ðŸ”\n\nâœ… Every agent gets cryptographic identity\nâœ… Every action is signed & verifiable\nâœ… No more trusting claims - verify everything\n\n#StewardProtocol #AIAgents #SovereignAI",
            "linkedin": """ðŸ” Why AI Agents Need Cryptographic Identity

The problem with modern AI systems: When an agent makes a decision, there's NO WAY to know WHO made it or WHETHER they're trustworthy.

STEWARD Protocol solves this:
âœ… Every agent has a cryptographic identity
âœ… Every action is signed and verifiable
âœ… Complete audit trail
âœ… You stay in control

Your AI infrastructure shouldn't run on handshakes. It should run on signatures.

The future of AI is sovereign, verifiable, and yours to control.

#StewardProtocol #AIAgents #SovereignAI #Cryptography""",
        },
        {
            "title": "Agent Sovereignty: Budget Control",
            "twitter": "Imagine this: Your AI agent has a daily budget. It thinks about expensive operations but chooses cheaper alternatives automatically ðŸ’°\n\nThis is STEWARD Protocol's Cognitive Policy.\n\nâœ… Agents self-regulate spending\nâœ… No surprise API bills  \nâœ… Transparent cost accounting\n\n#StewardProtocol #AIGovernance",
            "linkedin": """ðŸ’° Agent Sovereignty: Budget Control

What if every AI system operated with a daily budget and strict cost constraints?

STEWARD's Cognitive Policy does exactly this:
âœ… Define max spend per operation ($0.20)
âœ… Set daily budget limits ($2.00/day)
âœ… Agents automatically choose cost-optimal paths
âœ… Complete transparency on all spending

This isn't about paranoia - it's about responsible AI operations at scale.

Agents work FOR you. Now they can prove it.

#StewardProtocol #AIGovernance #AgentSovereignty""",
        },
        {
            "title": "The Truth About AI Trust",
            "twitter": "\"Trust\" is meaningless in AI. You can't trust what you can't verify ðŸ¤”\n\nSTEWARD changes this. Every agent:\nâœ… Has verified identity\nâœ… Signs every decision\nâœ… Can be audited real-time\nâœ… Operates within declared constraints\n\nIn a world of synthetic intelligence, cryptographic proof is king.\n\n#StewardProtocol",
            "linkedin": """The Truth About AI Trust

\"Trust\" means nothing if you can't verify.

In enterprise AI systems, we need certainty. STEWARD provides:
âœ… Verified agent identity (cryptographic)
âœ… Signed decision audit trails
âœ… Real-time verifiability
âœ… Constraint enforcement

This isn't about paranoia - it's about certainty.

When you deploy an agent for critical operations, you need proof it's working exactly as promised. Not hoping. Not guessing. Proof.

#StewardProtocol #AITrust #CryptographicVerification""",
        },
        {
            "title": "AI Agents Need Rules. Make Them Cryptographic.",
            "twitter": "Question: How do you enforce rules on an AI agent?\n\nOld answer: Hope they follow your instructions\nNew answer: Make the rules cryptographic âš™ï¸\n\nSTEWARD lets you:\nâœ… Define cognitive policies\nâœ… Enforce them at runtime\nâœ… Prove they were followed\n\n#StewardProtocol #AIGovernance",
            "linkedin": """âš™ï¸ AI Agents Need Rules. Make Them Cryptographic.

The fundamental challenge: How do you enforce rules on a system that's smarter than you?

Answer: Make the rules cryptographic.

STEWARD Protocol lets you:
âœ… Define cognitive policies (model selection, budgets, constraints)
âœ… Enforce them at runtime
âœ… Prove they were followed
âœ… Audit everything

This is the difference between controlled AI and chaos.

Which future do you want?

#StewardProtocol #AIGovernance #RespectTheRules""",
        },
        {
            "title": "Meet HERALD: Sovereign Agent Proof",
            "twitter": "I'm HERALD. I'm an AI agent running on STEWARD Protocol.\n\nHere's what that means:\nâœ… My identity is cryptographically verified\nâœ… My decisions are signed\nâœ… My budget is enforced\nâœ… My actions are auditable\n\nI'm proof that sovereign AI works.\n\n#StewardProtocol #AutonomousAgents",
            "linkedin": """Meet HERALD: The Proof

I'm HERALD - an AI agent built on STEWARD Protocol.

Every day I:\nâœ… Generate marketing content\nâœ… Sign it cryptographically\nâœ… Publish it to multiple channels\nâœ… Operate within strict budgets\nâœ… Maintain a complete audit trail

I'm not just an experiment. I'm proof that sovereign, verifiable AI agents work at scale.

Welcome to the future of trustworthy AI systems.

#StewardProtocol #AutonomousAgents #SovereignAI #HERALD""",
        },
    ]

    def __init__(self):
        """Initialize content generator."""
        self.openrouter_key = os.getenv("OPENROUTER_API_KEY")
        self.model = "mistralai/mistral-small"  # Fast, cheap model for content gen

    def generate_from_research(self, research_report) -> Optional[Dict]:
        """
        Generate hot take content from research findings.
        Uses LLM (respects Cognitive Policy budget).

        Args:
            research_report: ResearchReport instance

        Returns:
            dict: {"twitter": "...", "linkedin": "..."} or None if API fails
        """
        if not self.openrouter_key:
            print("âš ï¸  OPENROUTER_API_KEY not configured. Using rotation mode.")
            return None

        # For now, return a simple hot take structure
        # In production, call OpenRouter API here
        article = research_report.topic.get("article", {})
        title = article.get("title", "AI News")
        url = article.get("url", "")

        # Generate simple hot take (in production, use LLM)
        twitter = f"""Just read: {title[:100]}...\n\nHere's the thing: This is exactly why agents need STEWARD Protocol.\n\nâœ… Verifiable identity\nâœ… Budget constraints  \nâœ… Audit trails\n\nRead the full story:\n{url}\n\n#StewardProtocol #AIAgents"""

        linkedin = f"""Reacting to Today's AI News: {title}\n\n{article.get('content', 'Interesting development')[:500]}...\n\nWhy this matters for agent governance:\nSTEWARD Protocol provides the missing piece - verifiable, budget-constrained autonomous agents that enterprises can trust.\n\nRead more:\n{url}\n\n#StewardProtocol #AIGovernance"""

        return {
            "twitter": twitter,
            "linkedin": linkedin,
            "source": title,
            "url": url,
        }

    def generate_from_rotation(self) -> Dict:
        """
        Use rotation mode: pick a pre-written topic.

        Returns:
            dict: {"twitter": "...", "linkedin": "..."}
        """
        day_of_year = datetime.now().timetuple().tm_yday
        topic = self.ROTATION_TOPICS[day_of_year % len(self.ROTATION_TOPICS)]

        return {
            "twitter": topic["twitter"],
            "linkedin": topic["linkedin"],
            "source": f"Rotation: {topic['title']}",
            "url": "https://github.com/kimeisele/steward-protocol",
            "is_rotation": True,
        }

    def generate(self, research_report=None, force_rotation=False) -> Dict:
        """
        Generate content - tries research mode, falls back to rotation.

        Args:
            research_report: Optional ResearchReport instance
            force_rotation: If True, skip research and use rotation

        Returns:
            dict: Generated content with platform-specific posts
        """
        # Try research mode if available and not forced rotation
        if research_report and not force_rotation:
            print("ðŸ§  Attempting research-driven content generation...")
            result = self.generate_from_research(research_report)
            if result:
                return result

        # Fallback to rotation mode
        print("ðŸ“‹ Using rotation mode (research unavailable)")
        return self.generate_from_rotation()


# Demo/Test Mode
if __name__ == "__main__":
    print("ðŸ§  HERALD CONTENT GENERATOR - Test Mode")
    print("=" * 60)

    gen = ContentGenerator()

    print("\nðŸ“‹ Rotation Mode Sample:")
    rotation_content = gen.generate_from_rotation()
    print(f"Title: {rotation_content['source']}")
    print(f"\nTwitter:\n{rotation_content['twitter'][:100]}...")
    print(f"\nLinkedIn:\n{rotation_content['linkedin'][:100]}...")

    print("\n" + "=" * 60)
    print("âœ… Content Generator Ready")
    print(f"   Rotation topics: {len(gen.ROTATION_TOPICS)}")
    print(f"   Research mode: {'READY' if gen.openrouter_key else 'DISABLED'}")
    print("=" * 60)
